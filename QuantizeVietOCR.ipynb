{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from vietocr.tool.predictor import Predictor\n",
    "from vietocr.tool.config import Cfg\n",
    "from vietocr.tool.translate import build_model\n",
    "from torch import nn\n",
    "from vietocr.model.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(model, file_name):\n",
    "    output_path = './weights/'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)   \n",
    "    saved_path = os.path.join(output_path, file_name)\n",
    "    if os.path.exists(saved_path):\n",
    "        os.remove(saved_path)   \n",
    "    print('Save files in: ', saved_path)\n",
    "    torch.save(model.state_dict(), saved_path)\n",
    "    \n",
    "def save_torchscript_model(model, file_name):\n",
    "    output_path = './weights/'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)   \n",
    "    model_filepath = os.path.join(output_path, file_name)\n",
    "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
    "    print('Save in: ', model_filepath)\n",
    "    return model_filepath\n",
    "\n",
    "def load_torchscript_model(model_filepath, device):\n",
    "\n",
    "    model = torch.jit.load(model_filepath, map_location=device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gdown https://drive.google.com/uc?id=19QU4VnKtgm3gf0Uw_N2QKSquW1SQ5JiE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -qq -o ./data_line.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define config\n",
    "\n",
    "* *data_root*: the folder save your all images\n",
    "* *train_annotation*: path to train annotation\n",
    "* *valid_annotation*: path to valid annotation\n",
    "* *print_every*: show train loss at every n steps\n",
    "* *valid_every*: show validation loss at every n steps\n",
    "* *iters*: number of iteration to train your model\n",
    "* *export*: export weights to folder that you can use for inference\n",
    "* *metrics*: number of sample in validation annotation you use for computing full_sequence_accuracy, for large dataset it will take too long, then you can reuduce this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Cfg.load_config_from_name('vgg_seq2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = {\n",
    "    'name':'hw',\n",
    "    'data_root':'./data_line/',\n",
    "    'train_annotation':'train_line_annotation.txt',\n",
    "    'valid_annotation':'test_line_annotation.txt'\n",
    "}\n",
    "\n",
    "params = {\n",
    "         'print_every':200,\n",
    "         'valid_every':15*200,\n",
    "          'iters':100000,\n",
    "          'checkpoint':'./weights/transformerocr.pth',    \n",
    "          'export':'./weights/quantize_transformerocr.pth',\n",
    "          'metrics': 10000\n",
    "         }\n",
    "\n",
    "config['trainer'].update(params)\n",
    "config['dataset'].update(dataset_params)\n",
    "config['device'] = 'cuda:1'\n",
    "config['cnn']['pretrained']=False\n",
    "config['weights'] = \"./weights/transformerocr.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = config['device']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocab = build_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = config['weights']\n",
    "model.load_state_dict(torch.load(weights, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define input and outputs of quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedCNN(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedCNN, self).__init__()\n",
    "        \n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        \n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        \n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantize Aware Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Fuse layer\n",
    "\n",
    "Fuse 'conv + relu' or 'conv + batchnorm + relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()\n",
    "for m in model.cnn.model.modules():\n",
    "    if type(m) == nn.Sequential:\n",
    "        for n, layer in enumerate(m):\n",
    "            if type(layer) == nn.Conv2d:\n",
    "                torch.quantization.fuse_modules(m, [str(n), str(n + 1), str(n + 2)], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Prepare the model for quantization aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_cnn = QuantizedCNN(model_fp32=model.cnn)\n",
    "quantized_cnn.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "\n",
    "# Print quantization configurations\n",
    "print(quantized_cnn.qconfig)\n",
    "\n",
    "# the prepare() is used in post training quantization to prepares your model for the calibration step\n",
    "quantized_cnn = torch.quantization.prepare_qat(quantized_cnn, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cnn = quantized_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training\n",
    "\n",
    "Phụ thuộc vào bộ dữ liệu sử dụng huấn luyện sẽ dẫn đến kết quả khác nhau. Trong bài hướng dẫn này, mình sử dụng tạm thời bộ dữ liệu mẫu do thư viện VietOCR cung cấp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model = model.to(device)\n",
    "trainer = Trainer(qmodel=model, config=config, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dataset\n",
    "trainer.visualize_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Cfg.load_config_from_name('vgg_seq2seq')\n",
    "# Pytorch support only cpu device\n",
    "config['device'] = 'cpu'\n",
    "config['cnn']['pretrained']=False\n",
    "config['weights'] = \"./weights/quantize_transformerocr.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocab = build_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse layer\n",
    "model = model.train()\n",
    "for m in model.cnn.model.modules():\n",
    "    if type(m) == nn.Sequential:\n",
    "        for n, layer in enumerate(m):\n",
    "            if type(layer) == nn.Conv2d:\n",
    "                torch.quantization.fuse_modules(m, [str(n), str(n + 1), str(n + 2)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model for quantize aware training\n",
    "quantized_cnn = QuantizedCNN(model_fp32=model.cnn)\n",
    "quantized_cnn.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "\n",
    "# Print quantization configurations\n",
    "print(quantized_cnn.qconfig)\n",
    "\n",
    "# the prepare() is used in post training quantization to prepares your model for the calibration step\n",
    "quantized_cnn = torch.quantization.prepare_qat(quantized_cnn, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_cnn = quantized_cnn.to(torch.device('cpu'))\n",
    "model.cnn = torch.quantization.convert(quantized_cnn, inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create detector\n",
    "detector = Predictor(config, qmodel=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample image\n",
    "! gdown --id 1uMVd6EBjY4Q0G2IkU5iMOQ34X0bysm0b\n",
    "! unzip  -qq -o sample.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = './sample/031189003299.jpeg'\n",
    "img = Image.open(img)\n",
    "plt.imshow(img)\n",
    "s = detector.predict(img)\n",
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manhbq",
   "language": "python",
   "name": "manhbq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
